{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3wF5wszaj97Y"
   },
   "source": [
    "# < Deep Learning - PART2 TF2 CNNs >\n",
    "\n",
    "# Ch 5. CNNs Workshop 1- MNIST : Digit Recognition with Batch Normalization\n",
    "2021/10/01\n",
    "\n",
    "**[ Reference ]**\n",
    "1. TensorFlow Core - Tutorials : **`TensorFlow 2 quickstart for experts`**, 2019. https://www.tensorflow.org/tutorials/quickstart/advanced\n",
    "2. TensorFlow Core - Tutorials : **`Get started with TensorBoard`**, 2019. https://www.tensorflow.org/tensorboard/get_started\n",
    "3. IT邦幫忙, **`Day 17: Tensorflow 2.0 再造訪 tf.GradientTape`**, 2019/10/02. \n",
    "https://ithelp.ithome.com.tw/articles/10223779\n",
    "4. TensorFlow Core - Guides : **`Better performance with tf.function and AutoGraph`** \n",
    "https://www.tensorflow.org/guide/function\n",
    "5. TensorFlow Core - API : **`tf.keras.layers.BatchNormalization`**\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\n",
    "6. François Chollet, **Deep Learning with Python**, Section 7.3.1 (**BATCH NORMALIZATION**), pp. 260~261, Manning, 2018.\n",
    "http://www.deeplearningitalia.com/wp-content/uploads/2017/12/Dropbox_Chollet.pdf#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0trJmd6DjqBZ"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, BatchNormalization\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7NAbSZiaoJ4z"
   },
   "source": [
    "## 1. Load and prepare the `MNIST` dataset\n",
    "+ **`MNIST`** dataset - http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JqFRS6K07jJs"
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the data info..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 28, 28), (10000,))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train    #  y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 255)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.min(), x_train.max()    # x_test.min(), x_test.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.min(), x_train.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the data format into 4D Tensors for CNN's Inputs\n",
    "+ **4D Tensor : (samples, height, width, channels)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a \"channels\" dimension\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape  # x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `tf.data` to batch and shuffle the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k1Evqx0S22r_"
   },
   "source": [
    "+ **`tf.data.Dataset`** : https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "> `Class` **`Dataset`** \n",
    "        - Represents a potentially large set of elements.\n",
    "\n",
    "+ **Use `tf.data` to batch and shuffle the dataset**:\n",
    ">\n",
    "> For example :\n",
    ">\n",
    "> `train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))`\n",
    "     `.shuffle(10000).batch(32)`\n",
    "\n",
    "> [ NOTE ]:\n",
    "+ `tf.data.Dataset.from_tensor_slices` - https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices\n",
    "+ `shuffle()` - https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable#shuffle\n",
    "+ `batch()` - https://www.tensorflow.org/api_docs/python/tf/data/Dataset?version=stable#batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Iu_quO024c2"
   },
   "outputs": [],
   "source": [
    "# Nodes in the Computation Graph...\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(10000).batch(32)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float64, tf.uint8)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BPZ68wASog_I"
   },
   "source": [
    "## 2. Forward Propagation\n",
    "+ Build the `tf.keras` model using the Keras [model subclassing API](https://www.tensorflow.org/guide/keras#model_subclassing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h3IKyzTCDNGo"
   },
   "outputs": [],
   "source": [
    "class MyModel(Model):   # tf.keras.Model class\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = Conv2D(32, 3, activation='relu')  # tf.keras.layers.Conv2D()\n",
    "        self.batchnorm1 = BatchNormalization()   # tf.keras.layers.BatchNormalization()\n",
    "        self.flatten = Flatten()                 # tf.keras.layers.Flatten()\n",
    "        self.d1 = Dense(128, activation='relu')  # tf.keras.layers.Dense()\n",
    "        self.batchnorm2 = BatchNormalization()   # tf.keras.layers.BatchNormalization()\n",
    "        self.d2 = Dense(10)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        return self.d2(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MyModel at 0x197192a5220>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uGih-c2LgbJu"
   },
   "source": [
    "## 3. Backpropagation\n",
    "### Choosing an optimizer and loss function for training\n",
    "> **[Loss Function] :  `tf.keras.losses.SparseCategoricalCrossentropy()`** : https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy\n",
    "> + **Class `SparseCategoricalCrossentropy`** - Computes the crossentropy loss between the labels and predictions.\n",
    "    + **Use this crossentropy loss function when there are two or more label classes.**\n",
    "    + **We expect labels to be provided as integers.** \n",
    "    \n",
    "> [NOTE] : + **If you want to provide labels using `one-hot` representation, please use `CategoricalCrossentropy` loss.**\n",
    "    > + There should be `# classes` floating point values per feature for `y_pred` and a single floating point value per feature for `y_true`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u48C9WQ774n4"
   },
   "outputs": [],
   "source": [
    "#  < tf.keras.losses.SparseCategoricalCrossentropy >\n",
    "#    from_logits: Whether y_pred is expected to be a logits tensor. \n",
    "#                 By default, we assume that y_pred encodes \n",
    "#                 a probability distribution. \n",
    "#   [Note]: Using from_logits=True may be more numerically stable.\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JB6A1vcigsIe"
   },
   "source": [
    "### Selecting metrics to measure the loss and the accuracy of the model \n",
    "+ These metrics accumulate the values over epochs and then print the overall result.\n",
    "+ Module: **`tf.keras.metrics`**\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/metrics\n",
    "    > **Built-in metrics classes** : (*to name a few*)\n",
    "    > + **`class SparseCategoricalAccuracy`** : Calculates how often predictions matches integer labels. https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SparseCategoricalAccuracy\n",
    "    > + **`class CategoricalAccuracy`** : Calculates how often predictions matches labels.\n",
    "    > + **`class Mean`** : Computes the (weighted) mean of the given values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N0MqHFb4F_qn"
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ix4mEL65on-w"
   },
   "source": [
    "------------\n",
    "### Building A Training Model with `tf.function` & `tf.GradientTape`  \n",
    "+ Using `tf.function` decorator & `tf.GradientTape` method to build a custom-made training model instead of `tf.keras`'s`model.fit()`.\n",
    "+ **`tf.GradientTape()`** (https://www.tensorflow.org/api_docs/python/tf/GradientTape)\n",
    "    + When training with methods such as **`tf.GradientTape()`**, we can use **`tf.summary`** to log the required information.\n",
    "> + `tf.GradientTape()`可以用在 training loop 裡，記錄並建構正向傳播的計算圖。\n",
    "> + 在完成“記錄”後，tf.GradientTape() 的 tape 物件則呼叫 gradient()方法，並傳入損失值 (loss score) 和模型可訓練的參數。 [from **`Ref 3.`**]\n",
    "> + 一旦計算出了梯度後，立即呼叫 optimizer.apply_gradients() 方法，傳入一個 list of tuple，每一個 tuple 的第二個則是參數變數，而第一個變數為針對該參數所計算出的梯度。 [from **`Ref 3.`**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OZACiVqA8KQV"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):    # images : x_train , labels : y_train\n",
    "    ## ----------------------------------------------------------------------\n",
    "    ##  Forward propagation - \n",
    "    ##    tf.GradientTape()可以用在 training loop 裡，記錄並建構正向傳播的計算圖\n",
    "    ## ----------------------------------------------------------------------\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "\n",
    "    ## ----------------------------------------------------------------------\n",
    "    ##  Backpropagation - \n",
    "    ##    在完成“記錄”後，tf.GradientTape() 的 tape 物件則呼叫 gradient()方法，\n",
    "    ##    並傳入損失值 (loss score) 和模型可訓練的參數。 [from Ref 3.]\n",
    "    ## ----------------------------------------------------------------------\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    ## ----------------------------------------------------------------------\n",
    "    ##  Parameters' update - \n",
    "    ##    一旦計算出了梯度後，立即呼叫 optimizer.apply_gradients() 方法，\n",
    "    ##    傳入一個 list of tuple，每一個 tuple 的第二個則是參數變數，\n",
    "    ##    而第一個變數為針對該參數所計算出的梯度。 [from Ref 3.]\n",
    "    ## ----------------------------------------------------------------------    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z8YT7UmFgpjV"
   },
   "source": [
    "### Testing the model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xIKdEzHAJGt7"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TensorBoard with `tf.summary`\n",
    "+ Set up summary writers to write the summaries to disk in a different logs directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "log_directory = 'logs/CNN_MNIST_BN/'\n",
    "train_log_dir = log_directory + current_time + '/train'\n",
    "test_log_dir = log_directory + current_time + '/test'\n",
    "\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **[NOTE] :** \n",
    "+ **Use `tf.summary.scalar()` to log metrics (loss and accuracy) during training/testing within the scope of the summary writers to write the summaries to disk.** \n",
    "+ You have control over which metrics to log and how often to do it. \n",
    "+ **Other `tf.summary` functions enable logging other types of data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training & Testing Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i-2pkctU_Ci7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.13404148817062378, Accuracy: 96.05500030517578, Test Loss: 0.06725851446390152, Test Accuracy: 97.83999633789062\n",
      "Epoch 2, Loss: 0.05010765418410301, Accuracy: 98.44667053222656, Test Loss: 0.08503703773021698, Test Accuracy: 97.47000122070312\n",
      "Epoch 3, Loss: 0.02724248543381691, Accuracy: 99.12999725341797, Test Loss: 0.05962126702070236, Test Accuracy: 98.18999481201172\n",
      "Epoch 4, Loss: 0.019286485388875008, Accuracy: 99.35333251953125, Test Loss: 0.05750630050897598, Test Accuracy: 98.3699951171875\n",
      "Epoch 5, Loss: 0.013315939344465733, Accuracy: 99.58499908447266, Test Loss: 0.0671699196100235, Test Accuracy: 98.22999572753906\n",
      "Epoch 6, Loss: 0.010507801547646523, Accuracy: 99.63666534423828, Test Loss: 0.07832274585962296, Test Accuracy: 97.98999786376953\n",
      "Epoch 7, Loss: 0.007947785779833794, Accuracy: 99.73333740234375, Test Loss: 0.05841346085071564, Test Accuracy: 98.37999725341797\n",
      "Epoch 8, Loss: 0.006727313157171011, Accuracy: 99.78166961669922, Test Loss: 0.08063257485628128, Test Accuracy: 97.98999786376953\n",
      "Epoch 9, Loss: 0.006814904976636171, Accuracy: 99.7699966430664, Test Loss: 0.08273825794458389, Test Accuracy: 98.0999984741211\n",
      "Epoch 10, Loss: 0.006379015278071165, Accuracy: 99.79499816894531, Test Loss: 0.0773063600063324, Test Accuracy: 98.18999481201172\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "        \n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "        \n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(epoch+1,\n",
    "                          train_loss.result(),\n",
    "                          train_accuracy.result()*100,\n",
    "                          test_loss.result(),\n",
    "                          test_accuracy.result()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T4JfEh7kvx6m"
   },
   "source": [
    "The image classifier is now trained to ~98% accuracy on this dataset. To learn more, read the [TensorFlow tutorials](https://www.tensorflow.org/tutorials)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ###  To run TensorBoard, run the following command on `Anaconda (Powershell) Prompt` :\n",
    "`tensorboard --logdir=`_path/to/log-directory_\n",
    "\n",
    "> + For instance, **`tensorboard --logdir logs/CNN_MNIST_BN`**\n",
    "\n",
    "> Connecting to **`http://localhost:6006`**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "advanced.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "e9afd3253101e30c074f19e3615574f118d414e5211ab5a754e65723fada7756"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
