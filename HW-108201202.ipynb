{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 數學系 大學部 3B 108201202 謝佳穎\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, BatchNormalization, MaxPooling2D\n",
    "from tensorflow.keras import Model\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a \"channels\" dimension\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]\n",
    "x_train.shape  # x_test.shape\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(10000).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float64, tf.uint8)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(Model):   # tf.keras.Model class\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = Conv2D(32, (3,3), activation='relu')  # tf.keras.layers.Conv2D()\n",
    "        self.batchnorm1 = BatchNormalization()\n",
    "        self.conv2 = Conv2D(64,(3,3))  # tf.keras.layers.Conv2D()\n",
    "        self.batchnorm3 = BatchNormalization()\n",
    "        self.conv3 = Conv2D(128,(3,3))  # tf.keras.layers.Conv2D()\n",
    "        self.batchnorm4 = BatchNormalization()\n",
    "        self.maxpool = MaxPooling2D((2,2))\n",
    "        self.flatten = Flatten()                 # tf.keras.layers.Flatten()\n",
    "        self.d1 = Dense(128, activation='relu')  # tf.keras.layers.Dense()\n",
    "        self.batchnorm2 = BatchNormalization()   # tf.keras.layers.BatchNormalization()\n",
    "        self.d2 = Dense(10)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.batchnorm4(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        return self.d2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1053532138466835, Accuracy: 96.86833953857422, Test Loss: 0.06753943115472794, Test Accuracy: 98.07999420166016 , Running time:  121.40313863754272 seconds\n",
      "Epoch 2, Loss: 0.05247967690229416, Accuracy: 98.42333221435547, Test Loss: 0.04719925299286842, Test Accuracy: 98.38999938964844 , Running time:  117.86505484580994 seconds\n",
      "Epoch 3, Loss: 0.03893867880105972, Accuracy: 98.77666473388672, Test Loss: 0.038314178586006165, Test Accuracy: 98.72999572753906 , Running time:  118.23236322402954 seconds\n",
      "Epoch 4, Loss: 0.0314847007393837, Accuracy: 98.97999572753906, Test Loss: 0.05184536054730415, Test Accuracy: 98.47000122070312 , Running time:  118.28342652320862 seconds\n",
      "Epoch 5, Loss: 0.02516031637787819, Accuracy: 99.2316665649414, Test Loss: 0.03348919749259949, Test Accuracy: 99.06999969482422 , Running time:  118.3789553642273 seconds\n",
      "Epoch 6, Loss: 0.021158535033464432, Accuracy: 99.33000183105469, Test Loss: 0.039771534502506256, Test Accuracy: 98.72999572753906 , Running time:  118.76122903823853 seconds\n",
      "Epoch 7, Loss: 0.01676785573363304, Accuracy: 99.45166778564453, Test Loss: 0.03244715929031372, Test Accuracy: 99.08999633789062 , Running time:  118.44280672073364 seconds\n",
      "Epoch 8, Loss: 0.013025353662669659, Accuracy: 99.58999633789062, Test Loss: 0.03070889227092266, Test Accuracy: 99.0199966430664 , Running time:  118.44040703773499 seconds\n",
      "Epoch 9, Loss: 0.010854368098080158, Accuracy: 99.63666534423828, Test Loss: 0.060035694390535355, Test Accuracy: 98.44999694824219 , Running time:  118.54960227012634 seconds\n",
      "Epoch 10, Loss: 0.009264071471989155, Accuracy: 99.69000244140625, Test Loss: 0.04272313043475151, Test Accuracy: 98.79000091552734 , Running time:  118.53286647796631 seconds\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the model\n",
    "model = MyModel()\n",
    "model\n",
    "#  < tf.keras.losses.SparseCategoricalCrossentropy >\n",
    "#    from_logits: Whether y_pred is expected to be a logits tensor. \n",
    "#                 By default, we assume that y_pred encodes \n",
    "#                 a probability distribution. \n",
    "#   [Note]: Using from_logits=True may be more numerically stable.\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "@tf.function\n",
    "def train_step(images, labels):    # images : x_train , labels : y_train\n",
    "    ## ----------------------------------------------------------------------\n",
    "    ##  Forward propagation - \n",
    "    ##    tf.GradientTape()可以用在 training loop 裡，記錄並建構正向傳播的計算圖\n",
    "    ## ----------------------------------------------------------------------\n",
    "    with tf.GradientTape() as tape:\n",
    "        # training=True is only needed if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "\n",
    "    ## ----------------------------------------------------------------------\n",
    "    ##  Backpropagation - \n",
    "    ##    在完成“記錄”後，tf.GradientTape() 的 tape 物件則呼叫 gradient()方法，\n",
    "    ##    並傳入損失值 (loss score) 和模型可訓練的參數。 [from Ref 3.]\n",
    "    ## ----------------------------------------------------------------------\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    ## ----------------------------------------------------------------------\n",
    "    ##  Parameters' update - \n",
    "    ##    一旦計算出了梯度後，立即呼叫 optimizer.apply_gradients() 方法，\n",
    "    ##    傳入一個 list of tuple，每一個 tuple 的第二個則是參數變數，\n",
    "    ##    而第一個變數為針對該參數所計算出的梯度。 [from Ref 3.]\n",
    "    ## ----------------------------------------------------------------------    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    # training=False is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "log_directory = 'logs/CNN_MNIST_BN/'\n",
    "train_log_dir = log_directory + current_time + '/train'\n",
    "test_log_dir = log_directory + current_time + '/test'\n",
    "\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    import time\n",
    "    start=time.time()\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "        \n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "        \n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "    end=time.time()\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(epoch+1,\n",
    "                          train_loss.result(),\n",
    "                          train_accuracy.result()*100,\n",
    "                          test_loss.result(),\n",
    "                          test_accuracy.result()*100),\", Running time: \", str(end-start), \"seconds\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e9afd3253101e30c074f19e3615574f118d414e5211ab5a754e65723fada7756"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
